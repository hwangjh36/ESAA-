{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0927_세션필사.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OJf_zlAPip7XDA906cyNqIckph3NNLQ7",
      "authorship_tag": "ABX9TyNENeHhv52KbIvSCnSC4P6o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vldzmtnsdl/ESAA-/blob/ESAA/0927_%EC%84%B8%EC%85%98%ED%95%84%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JaLsiGq9Cve"
      },
      "source": [
        "### [NLP 언제까지 미룰래? 일단 들어와!!]4. word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaoxCYMj1vww"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHji9T561tkz"
      },
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/train.csv')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfsQ4TO1hmr",
        "outputId": "622a3444-a014-422c-ca2f-67e1926497d0"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  42331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6AkwMEYyDG_"
      },
      "source": [
        "1.keras embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTrTstUTuHaC"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.layers import Embedding\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpniyPpO4QOL"
      },
      "source": [
        "max_len=100"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV4oRmEBt074"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,128,input_length=max_len))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aghco4gjyHZw"
      },
      "source": [
        "2. word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKiaI8-PxrX0"
      },
      "source": [
        "import gensim\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary = True)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97yp4IGkxcsI"
      },
      "source": [
        "embedding_matrix=np.zeros((vocab_size,300))\n",
        "\n",
        "for index,word in enumerate(tokenizer.word_index):\n",
        "  if word in word2vec:\n",
        "    embedding_vector=word2vec[word]\n",
        "    embedding_matrix[index]=embedding_vector\n",
        "  else:\n",
        "    print('word2vec에 없는 단어입니다.')\n",
        "    break"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcnon4J4yupe"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_len))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsJuQDxR4dcs"
      },
      "source": [
        "3.glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3V9286xy5Rg"
      },
      "source": [
        "glove=dict()\n",
        "f=open('/content/drive/MyDrive/glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.array(values[1:], dtype='float32')\n",
        "    glove[word] = vector\n",
        "f.close()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlMoyFna1VTY"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 300)) \n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): \n",
        "    if word in glove: \n",
        "        embedding_vector = glove[word] \n",
        "        embedding_mxtrix[i] = embedding_vector\n",
        "    else:\n",
        "        print(\"glove 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdlL_Biw41JS"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bujFTMRY5qg4"
      },
      "source": [
        "4. fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1yXp5fh5meJ"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "FastText = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/fasttext.vec', binary=True, unicode_errors='ignore')\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNWoSgif502i"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for index, word in enumerate(tokenizer.word_index): \n",
        "    if word in word2vec: \n",
        "        embedding_vector = word2vec[word] \n",
        "        embedding_mxtrix[i] = embedding_vector \n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svWxfMxD9KDt"
      },
      "source": [
        "## [NLP 언제까지 미룰래? 일단 들어와!!] 5. 대회적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4fOj5iN6gka"
      },
      "source": [
        "import tqdm as tqdm\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2_qV6SBDsXJ",
        "outputId": "f8ee59ae-de82-46d3-f187-71ec3cb0eeaf"
      },
      "source": [
        "pip install konlpy\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 22.3 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUYfyhSlEZ8n"
      },
      "source": [
        "1. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr1rCL4wCvNy",
        "outputId": "2ce25577-2d2a-4a25-a6eb-2b10c40787fb"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import tqdm \n",
        "\n",
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
        "    tokenizer = Okt() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for text in tqdm.tqdm(text_list):\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
        "        token = tokenizer.morphs(txt) #형태소 분석\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n",
        "    \n",
        "train['token'], okt = text_preprocessing(train['text'])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54879/54879 [01:27<00:00, 623.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYIIQzJHEcj6"
      },
      "source": [
        "2. vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbEylEJjDnxf",
        "outputId": "41a77d8a-1994-4674-921a-d74d3b6bb65d"
      },
      "source": [
        "def text2sequence(train_text,max_len=1000):\n",
        "  tokenizer=Tokenizer()\n",
        "  tokenizer.fit_on_texts(train_text)\n",
        "  train_X_seq=tokenizer.texts_to_sequences(train_text)\n",
        "  vocab_size=len(tokenizer.word_index)+1\n",
        "  print('vocab_size:',vocab_size)\n",
        "  X_train=pad_sequences(train_X_seq,maxlen=max_len)\n",
        "  return X_train,vocab_size,tokenizer\n",
        "\n",
        "train_y=train['author']\n",
        "train_X,vocab_szie,vectorizer=text2sequence(train['token'],max_len=100)\n",
        "print(train_X.shape,train_y.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 36342\n",
            "(54879, 100) (54879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-om8L4FZFI"
      },
      "source": [
        "3.Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3A9u_r_FVie"
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index):\n",
        "    if word in word2vec:\n",
        "        embedding_vector = word2vec[word] \n",
        "        embedding_matrix[index] = embedding_vector \n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs4745hmFquI"
      },
      "source": [
        "4. modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0cb8jlJFjYU"
      },
      "source": [
        "def LSTM(vocab_size, max_len=1000):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) \n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    model.summary()\n",
        "    return mode"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzNXjykHFxmw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}